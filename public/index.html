<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css"
      rel="stylesheet"
    />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css"
    />
    <title>Cameron Olson | Portfolio</title>
    <link rel="icon" href="images/ICON2.png" type="image/png" />
    <link rel="stylesheet" href="styles.css" />
  </head>
  <body>
    <!-- Main Header -->
    <header class="p-5 text-white text-center">
      <h1>Cameron Olson</h1>
      <h3>Audio Deep Learning</h3>
      <a
        href="https://github.com/cmsolson75"
        target="_blank"
        class="text-white"
      >
        <i class="fab fa-github"></i> GitHub
      </a>
    </header>
    <!-- Nav Bar -->
    <nav class="text-center">
      <a href="#biography">Biography</a>
      <a href="#projects">Projects</a>
      <a href="#music">Music</a>
      <a href="#education">Education</a>
      <a href="#contact">Contact</a>
      <a
        href="documents/Cameron_Olson_Resume.pdf"
        download="cameron_olson_resume"
        target="_blank"
        >Resume</a
      >
    </nav>
    <!-- Bio Section -->
    <section class="p-5" id="biography">
      <div class="container">
        <h2 class="text-center">Biography</h2>
        <p>
          Cameron Olson, a computer musician, coder, and recent graduate,
          seamlessly blends his passion for music and technology, particularly
          within the realm of Artificial Intelligence. With a specialization in
          Audio Deep Learning, Cameron harnesses AI to create innovative
          auditory experiences, notably through his project, "Almost Human,"
          which crafts AI-powered music that challenges and expands the
          boundaries of traditional composition and production. His commitment
          to making art more connected and accessible through technology
          resonates not only in his creations but also in his steadfast belief
          that everyone, including those with learning disabilities like him,
          deserves the right to self-expression. Although Cameron has concluded
          his academic journey at Berklee College of Music, where he delved
          deeply into electronic production and design, his expertise in Python
          and multifaceted experience in music—spanning composition, sound
          design, production, and audio delivery—continue to propel his
          professional and artistic explorations. His rich history as a jazz
          drummer, with five years of professional playing under his belt,
          underscores his versatile musicianship and informs his innovative
          approach to melding technological and musical realms.
        </p>
      </div>
    </section>
    <!-- Project Cards -->
    <section class="p-5 bg-light" id="projects">
      <div class="container">
        <h2 class="text-center mb-4">Projects</h2>
        <div class="row">
          <!-- Exploring Audio Diffusion Synthesis Card -->
          <article class="col-md-6">
            <div class="card">
              <!-- Image -->
              <div
                class="project-image"
                style="background-image: url('images/Life.png')"
              ></div>
              <!-- Card Body -->
              <div class="card-body">
                <!-- Title -->
                <h4>Exploring Audio Diffusion Synthesis</h4>
                <!-- Project Subscript -->
                <p>
                  Undergraduate Thesis | Berklee College of Music | Advisor: Dr.
                  Richard Boulanger
                </p>
                <!-- Card Text -->
                <p>
                  My undergraduate thesis, 'Exploring Audio Diffusion
                  Synthesis,' offers a thorough analysis of Raw Audio Neural
                  Network Generation, exploring the adaptation and application
                  of novel AI technologies to satisfy musicians' requirements
                  and innovate in sound design. The project blends theory and
                  practice, supporting findings with a research paper and a
                  dataset of 20,000+ drum samples. I also created music to
                  showcase the practical and creative possibilities of the AI
                  models in sound design.
                </p>
                <!-- Button to trigger modal -->
                <button
                  type="button"
                  class="btn btn-dark"
                  data-bs-toggle="modal"
                  data-bs-target="#project1Modal"
                >
                  Learn More
                </button>
              </div>
            </div>
          </article>

          <!-- Almost Human Card -->
          <article class="col-md-6">
            <div class="card">
              <!-- Image -->
              <div
                class="project-image"
                style="background-image: url('images/HANAUMAM.png')"
              ></div>
              <!-- Card Body -->
              <div class="card-body">
                <!-- Title -->
                <h4>Almost Human</h4>
                <!-- Project Subscript -->
                <p>Artist project | AI Song Competition</p>
                <!-- Card Text -->
                <p>
                  "Almost Human," an innovative artistic endeavor initiated at
                  Berklee College of Music, delves deep into the intricate
                  interplay between Machine Learning and the Artist. This
                  collective is propelled by the conviction that the future of
                  music resides at the crossroads of cutting-edge technology and
                  human creativity. We aspire to democratize the sheer joy of
                  music creation, ensuring that this exhilarating experience is
                  open and available to everyone.
                </p>
                <!-- Button trigger modal -->
                <button
                  type="button"
                  class="btn btn-dark"
                  data-bs-toggle="modal"
                  data-bs-target="#project2Modal"
                >
                  Learn More
                </button>
              </div>
            </div>
          </article>

          <!-- Catch-A-Waveform Card -->
          <article class="col-md-6">
            <div class="card">
              <!-- Image -->
              <div
                class="project-image"
                style="background-image: url('images/Creation.png')"
              ></div>
              <!-- Card Body -->
              <div class="card-body">
                <!-- Title -->
                <h4>Catch-A-Waveform: Google Colab Notebook</h4>
                <!-- Project Subscript -->
                <p>Google Colaboratory Project | Deep Learning | Open Source</p>
                <!-- Card Text -->
                <p>
                  "Catch-A-Waveform" is a project primarily focused on audio
                  generation and manipulation, with capabilities to generate
                  audio from a single short example. This is a full Google Colab
                  build of a modified Catch-A-Waveform repository from the
                  Dadabots' Zack Zukowski.
                </p>
                <!-- Button trigger modal -->
                <button
                  type="button"
                  class="btn btn-dark"
                  data-bs-toggle="modal"
                  data-bs-target="#project3Modal"
                >
                  Learn More
                </button>
              </div>
            </div>
          </article>

          <!-- MNIST Card -->
          <article class="col-md-6">
            <div class="card">
              <!-- Image -->
              <div
                class="project-image"
                style="background-image: url('images/IntoTheVoid.png')"
              ></div>
              <!-- Card Body -->
              <div class="card-body">
                <!-- Title -->
                <h4>MNIST Deployment Project</h4>
                <!-- Project Subscript -->
                <p>PyTorch Project | Model Deployment</p>
                <!-- Card Text -->
                <p>
                  The MNIST Hand-Drawn Digit Recognition project is a
                  Python-based application that utilizes Deep Learning to
                  recognize hand-drawn digits. The project is structured to
                  train a neural network model on the MNIST dataset and
                  subsequently provide a graphical user interface (GUI) for
                  real-time digit recognition.
                </p>
                <!-- Button to trigger modal -->
                <button
                  type="button"
                  class="btn btn-dark"
                  data-bs-toggle="modal"
                  data-bs-target="#project4Modal"
                >
                  Learn More
                </button>
              </div>
            </div>
          </article>

          <!-- Youtube Playlist Card -->
          <article class="col-md-6">
            <div class="card">
              <!-- Image -->
              <div
                class="project-image"
                style="background-image: url('images/BreakFree.png')"
              ></div>
              <!-- Card Body -->
              <div class="card-body">
                <!-- Title -->
                <h4>Youtube Playlist Length Calculator</h4>
                <!-- Project Subtext -->
                <p>YouTube Data API v3 Project | Pair Programming</p>
                <!-- Card Text -->
                <p>
                  The YouTube Playlist Length Calculator is a Python script
                  designed to calculate the total duration of all videos within
                  a specified YouTube playlist. This can be particularly useful
                  for assessing the total watch time of course material,
                  tutorial series, or any collection of videos grouped into a
                  YouTube playlist.
                </p>
                <!-- Button to trigger modal -->
                <button
                  type="button"
                  class="btn btn-dark"
                  data-bs-toggle="modal"
                  data-bs-target="#project5Modal"
                >
                  Learn More
                </button>
              </div>
            </div>
          </article>

          <!-- Socket Morse Chat Card -->
          <article class="col-md-6">
            <div class="card">
              <!-- Image -->
              <div
                class="project-image"
                style="background-image: url('images/Reality.png')"
              ></div>
              <!-- Card Body -->
              <div class="card-body">
                <!-- Title -->
                <h4>Socket Morse Chat App</h4>
                <!-- Project Subscript -->
                <p>Socket Programming Project | Client-Server Architecture</p>
                <!-- Card Text -->
                <p>
                  The Socket Morse Chat App is a communication application
                  developed in Python that allows users to send and receive
                  messages in Morse code through a client-server architecture.
                  The application translates text messages into Morse code and
                  plays the corresponding audio to the users.
                </p>
                <!-- Button to trigger modal -->
                <button
                  type="button"
                  class="btn btn-dark"
                  data-bs-toggle="modal"
                  data-bs-target="#project6Modal"
                >
                  Learn More
                </button>
              </div>
            </div>
          </article>
        </div>
      </div>
    </section>

    <!-- Modal for Exploring Audio Diffusion Synthesis -->
    <div
      class="modal fade"
      id="project1Modal"
      tabindex="-1"
      aria-labelledby="project1ModalLabel"
      aria-hidden="true"
    >
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="modal-header">
            <!-- Title -->
            <h5 class="modal-title" id="project1ModalLabel">
              Exploration of Audio Diffusion Synthesis
            </h5>
            <!-- Close Button -->
            <button
              type="button"
              class="btn-close"
              data-bs-dismiss="modal"
              aria-label="Close"
            ></button>
          </div>
          <!-- Modal Body -->
          <div class="modal-body">
            <!-- Overview Section -->
            <h5>Overview</h5>
            <p>
              My undergraduate thesis, 'Exploring Audio Diffusion Synthesis,'
              offers a thorough analysis of Raw Audio Neural Network Generation,
              exploring the adaptation and application of novel AI technologies
              to satisfy musicians' requirements and innovate in sound design.
              The project blends theory and practice, supporting findings with a
              research paper and a dataset of 20,000+ drum samples. I also
              created music to showcase the practical and creative possibilities
              of the AI models in sound design.
            </p>
            <!-- Key Points Section -->
            <h5>Key Points</h5>
            <ul>
              <li>
                <strong>Dataset Assembly:</strong> Compiled a robust dataset of
                20,000+ drum samples, ensuring a comprehensive basis for
                subsequent neural network training and experimental validation.
              </li>
              <li>
                <strong>AI Model Development and Application:</strong>
                Engineered and trained AI models in the domain of Audio
                Diffusion Synthesis, utilizing them to generate music and
                exemplify their pragmatic and innovative applications in sound
                design.
              </li>
              <li>
                <strong>Research Synthesis:</strong> Conducted rigorous research
                into Raw Audio Neural Network Generation, culminating in a
                scholarly research paper that encapsulates the findings and
                theoretical underpinnings of the explored technologies.
              </li>
              <li>
                <strong>Technological Exploration:</strong> Investigated and
                implemented emerging AI technologies, tailoring them to address
                specific requisites of musicians and to innovate in the field of
                sound design, with a particular focus on diffusion synthesis
                using the open-source Dance Diffusion model by Harmonai for a
                large portion of my research.
              </li>
              <li>
                <strong>Presentation of Findings:</strong> Articulated research
                findings and practical applications through a multifaceted
                presentation approach, encompassing a research paper, a showcase
                video, and a master class, targeting both academic and
                practitioner audiences at Berklee.
              </li>
            </ul>
            <!-- Paper Abstract -->
            <h5>Abstract</h5>
            <p>
              This research dives into Audio Diffusion Synthesis, exploring the
              potential of Neural Networks in sound generation. Specifically
              focusing on the Dance Diffusion architecture, the paper navigates
              through system selection, data collection, and model training,
              demonstrating the effectiveness of DDPMs in generating
              authentic-sounding audio signals, providing valuable insights into
              the future of audio synthesis in music production.
            </p>
            <!-- Link Section -->
            <h5>Explore Further</h5>
            <ul>
              <!-- Youtube Video Link -->
              <li>
                <strong>Youtube Video:</strong>
                <a
                  href="https://youtu.be/ijDwHkTExJk?si=n2lA_CEbRnDzv92s"
                  target="_blank"
                  >Audio Diffusion Synthesis Showcase</a
                >
              </li>
              <!-- Research Paper Link -->
              <li>
                <strong>Research Paper:</strong>
                <a
                  href="documents/Exploring_Audio_Diffusion_Synthesis.pdf"
                  target="_blank"
                  download="Research_Paper"
                  >Audio Diffusion Synthesis Paper</a
                >
              </li>
              <!-- Link to music section -->
              <li>
                <strong>Musical Example:</strong>
                <a href="#music" class="my-link" data-bs-dismiss="modal"
                  >Stay With Me</a
                >
              </li>
            </ul>
          </div>
          <!-- Footer Class for adding a bottom line -->
          <div class="modal-footer"></div>
        </div>
      </div>
    </div>

    <!-- Modal for Almost Human -->
    <div
      class="modal fade"
      id="project2Modal"
      tabindex="-1"
      aria-labelledby="project2ModalLabel"
      aria-hidden="true"
    >
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="modal-header">
            <!-- Title -->
            <h5 class="modal-title" id="project2ModalLabel">Almost Human</h5>
            <!-- Close Button -->
            <button
              type="button"
              class="btn-close"
              data-bs-dismiss="modal"
              aria-label="Close"
            ></button>
          </div>
          <!-- Modal Body -->
          <div class="modal-body">
            <!-- Overview Section -->
            <h5>Overview</h5>
            <p>
              "Almost Human," an innovative artistic endeavor initiated at
              Berklee College of Music, delves deep into the intricate interplay
              between Machine Learning and the Artist. This collective is
              propelled by the conviction that the future of music resides at
              the crossroads of cutting-edge technology and human creativity. We
              aspire to democratize the sheer joy of music creation, ensuring
              that this exhilarating experience is open and available to
              everyone.
            </p>
            <!-- Technical Blueprint Section -->
            <strong>Technical Blueprint:</strong>
            <ul>
              <li>
                <strong>Lyrics:</strong> Generated with GPT-4 and a fine tuned
                Llama2 on lyrics, Llama2 generating the name "I'm a Little Too
                Young to Be This Old" and GPT-4 generating lyrics from that
                starting place.
              </li>
              <li>
                <strong>MIDI Generation:</strong> Employed a Transformer XL
                Architecture trained on the Lakh midi dataset.
              </li>
              <li>
                <strong>Vocals:</strong> Generated with
                <a href="http://kits.ai/" target="_blank">Kits.ai</a> vocal
                style transfer models fine-tuned with custom vocal data from
                <a href="https://splice.com/" target="_blank">splice.com</a>.
                Original vocal performance by Faith Manning.
              </li>
              <li>
                <strong>Sound Design & Sample Generation:</strong> All generated
                with Harmonai’s Dance Diffusion.

                <ul>
                  <li>
                    <strong>Drum Samples:</strong> Leveraged a 20,000 drum
                    sample dataset from my Undergraduate Thesis Project.
                  </li>
                  <li>
                    <strong>Bird Samples:</strong> Recordings from the serene
                    parks of Seattle, Washington.
                  </li>
                  <li>
                    <strong>Instrument Samples:</strong> Multiple models refined
                    on instrument data from
                    <a href="https://splice.com/" target="_blank">splice.com</a
                    >, creating diverse sonic textures.
                  </li>
                  <li>
                    <strong>Glitch Samples:</strong> A collection of 300 unique
                    glitch samples sourced from
                    <a href="https://splice.com/" target="_blank">splice.com</a
                    >, adding a contemporary edge.
                  </li>
                  <li>
                    <strong>Bass Growls:</strong>Utilizing 200 bass growls from
                    <a href="https://splice.com/" target="_blank">splice.com</a
                    >, adding intensity in the drop.
                  </li>
                </ul>
              </li>
            </ul>
            <!-- AI Song Contest Entry Section -->
            <h5>AI Song Contest Entry</h5>
            <p>
              The piece encapsulates the vision of AI as a bridge in the vast
              landscape of music. It stands as a testament to AI's capability to
              serve beginners, empower those with disabilities, and inspire even
              the most seasoned musicians. This composition is not just a melody
              but a representation of the inclusive and innovative power of AI
              in music.
            </p>
            <!-- Links -->
            <h5>Explore Further</h5>
            <li>
              <strong>Musical Example:</strong>
              <a href="#music" class="my-link-2" data-bs-dismiss="modal"
                >I'm A Little Too Young to Be This Old</a
              >
            </li>
            <!-- Link to AI Song Contest Page -->
            <li>
              <strong>AI Song Contest Page:</strong>
              <a
                href="https://www.aisongcontest.com/participants-2023/almost-human"
                target="_blank"
                >Almost Human</a
              >
            </li>
          </div>
          <!-- Footer for adding line at bottom -->
          <div class="modal-footer"></div>
        </div>
      </div>
    </div>

    <!-- Modal for Catch-A-Waveform Colab Notebook -->
    <div
      class="modal fade"
      id="project3Modal"
      tabindex="-1"
      aria-labelledby="project3ModalLabel"
      aria-hidden="true"
    >
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="modal-header">
            <!-- Title -->
            <h5 class="modal-title" id="projec31ModalLabel">
              Catch-A-Waveform Colab Notebook
            </h5>
            <!-- Exit Button -->
            <button
              type="button"
              class="btn-close"
              data-bs-dismiss="modal"
              aria-label="Close"
            ></button>
          </div>
          <!-- Modal Body -->
          <div class="modal-body">
            <!-- Overview -->
            <h5>Overview</h5>
            <p>
              "Catch-A-Waveform" is a project primarily focused on audio
              generation and manipulation, with capabilities to generate audio
              from a single short example. This is a full Google Colab build of
              a modified Catch-A-Waveform repository from the Dadabots' Zack
              Zukowski.
            </p>
            <!-- Key Features -->
            <h5>Key Features</h5>
            <ul>
              <li>
                <strong>Google Colaboratory Integration:</strong>
                <ul>
                  <li>
                    Utilize Cloud GPU resources for faster model training.
                  </li>
                </ul>
              </li>
              <li>
                <strong>Google Drive Mounting:</strong>
                <ul>
                  <li>
                    Conveniently access and manage project data stored in Google
                    Drive.
                  </li>
                </ul>
              </li>
              <li>
                <strong>No Code Setup:</strong>
                <ul>
                  <li>
                    Clone the project's GitHub repository effortlessly,
                    requiring minimal coding knowledge.
                  </li>
                </ul>
              </li>
              <li>
                <strong>Model Management:</strong>
                <ul>
                  <li>
                    Model Checkpointing: Easily save and restore model
                    checkpoints, ensuring seamless training progress tracking.
                  </li>
                </ul>
              </li>
              <li>
                <strong>Enhanced Catch-A-Waveform Capabilities:</strong>
                <ul>
                  <li>
                    <strong>Training Run Modes:</strong>
                    <ul>
                      <li>
                        Introduces new training run modes, including resume and
                        transfer, for improved model training flexibility.
                      </li>
                    </ul>
                  </li>
                  <li>
                    <strong>Generating Run Mode:</strong>
                    <ul>
                      <li>
                        Introduces a new generating run mode called "infinite"
                        for generating audio continuously.
                      </li>
                    </ul>
                  </li>
                  <li>
                    <strong>Audio Sampling:</strong>
                    <ul>
                      <li>
                        Implements a new audio sampling method called
                        "--scale_crop" to efficiently fit maximum audio data
                        into memory at different scales.
                      </li>
                    </ul>
                  </li>
                  <li>
                    <strong>Model Building:</strong>
                    <ul>
                      <li>
                        Allows the creation of models with skip connections in
                        the 1D dilated convolution stacks, enhancing model
                        complexity and performance.
                      </li>
                    </ul>
                  </li>
                  <li>
                    <strong>Hyperparameters:</strong>
                    <ul>
                      <li>
                        Adds new hyperparameters tailored for complex music
                        generation, expanding the range from 250Hz to 44.1kHz.
                      </li>
                    </ul>
                  </li>
                  <li>
                    <strong>Conditioning Options:</strong>
                    <ul>
                      <li>
                        Offers options to condition audio generation on any
                        input audio file using "--condition_file" and customize
                        the frequency scale with "--condition_fs."
                      </li>
                    </ul>
                  </li>
                  <li>
                    <strong>Lite Training:</strong>
                    <ul>
                      <li>
                        Includes an experimental "lite training" mode with
                        precision-reduced optimizers to conserve memory
                        resources during training.
                      </li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
            <!-- Links -->
            <h5>Explore Further:</h5>
            <ul>
              <!-- Link to original project -->
              <li>
                <strong>Original Project:</strong>
                <a
                  href="https://galgreshler.github.io/Catch-A-Waveform/#"
                  target="_blank"
                  >Catch-A-Waveform</a
                >
              </li>
              <!-- Notebook link -->
              <li>
                <strong>Notebook:</strong>
                <a
                  href="https://colab.research.google.com/drive/1bo-SjlCFLZL1mR4XzwAv6nO_R1AXMNR6?usp=drive_link"
                  target="_blank"
                  >Google Colab Notebook</a
                >
              </li>
            </ul>
          </div>
          <!-- Footer for adding line at bottom -->
          <div class="modal-footer"></div>
        </div>
      </div>
    </div>

    <!-- Modal for MNIST GUI Project -->
    <div
      class="modal fade"
      id="project4Modal"
      tabindex="-1"
      aria-labelledby="project4ModalLabel"
      aria-hidden="true"
    >
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="modal-header">
            <!-- Title -->
            <h5 class="modal-title" id="project4ModalLabel">
              MNIST GUI Deployment
            </h5>
            <!-- Exit Button -->
            <button
              type="button"
              class="btn-close"
              data-bs-dismiss="modal"
              aria-label="Close"
            ></button>
          </div>
          <!-- Modal Body -->
          <div class="modal-body">
            <div class="card-body">
              <!-- Overview Section -->
              <h5>Overview</h5>
              <p>
                The MNIST Hand-Drawn Digit Recognition project is a Python-based
                application that utilizes Deep Learning to recognize hand-drawn
                digits. The project is structured to train a neural network
                model on the MNIST dataset and subsequently provide a graphical
                user interface (GUI) for real-time digit recognition.
              </p>
              <!-- Key Technology Section -->
              <h5>Key Technology</h5>
              <ul>
                <li>
                  <strong>Python: </strong>the project uses Python as the
                  primary programming language.
                </li>
                <li>
                  <strong>Jupyter Notebooks: </strong>Used for data exploration
                  and model training.
                </li>
                <li>
                  <strong>PyTorch: </strong>Used for model training and
                  inference.
                </li>
                <li>
                  <strong>Tkinter: </strong> Used for GUI application to run
                  model inference.
                </li>
                <li>
                  <strong>Pytest: </strong> Testing ensures for functionality
                  code safety.
                </li>
                <li>
                  <strong>Numpy & Matplotlib: </strong>Used for Data
                  Visualization and Analysis.
                </li>
              </ul>
              <!-- Deep dive into GUI Application -->
              <h5>GUI Application</h5>
              <p>
                The GUI application, developed in Python and housed in the
                gui.py file, provides an interactive platform for digit
                recognition using a pre-trained neural network model. Users can
                draw digits on a 280x280 pixel canvas, and upon clicking
                "Predict", the drawn digit is resized, preprocessed, and fed to
                the model, displaying the predicted digit on the interface. The
                application also allows users to clear the canvas and draw new
                digits for prediction. It ensures a user-friendly experience by
                providing clear controls for drawing, predicting, and clearing,
                along with handling potential errors, such as model loading
                issues, gracefully by informing the user through clear error
                messages. This application demonstrates a practical, real-world
                application of a machine learning model in a manner that is
                accessible and interactive for users without technical expertise
                in machine learning.
              </p>

              <!-- Links -->
              <h5>Explore Further</h5>
              <ul>
                <!-- Link to GitHub Repo -->
                <li>
                  <strong>GitHub Repository:</strong>
                  <a
                    href="https://github.com/cmsolson75/MNIST_GUI_Project"
                    target="_blank"
                    >MNIST_GUI_Project</a
                  >
                </li>
              </ul>
            </div>
            <!-- Footer for adding line at bottom -->
            <div class="modal-footer"></div>
          </div>
        </div>
      </div>
    </div>

    <!-- Modal for Youtube Playlist Length Calculator -->
    <div
      class="modal fade"
      id="project5Modal"
      tabindex="-1"
      aria-labelledby="project5ModalLabel"
      aria-hidden="true"
    >
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="modal-header">
            <!-- Title -->
            <h5 class="modal-title" id="project5ModalLabel">
              Youtube Playlist Length Calculator
            </h5>
            <!-- Exit Button -->
            <button
              type="button"
              class="btn-close"
              data-bs-dismiss="modal"
              aria-label="Close"
            ></button>
          </div>
          <!-- Modal Body -->
          <div class="modal-body">
            <!-- Overview Section -->
            <h5>Overview</h5>
            <p>
              The YouTube Playlist Length Calculator is a Python script designed
              to calculate the total duration of all videos within a specified
              YouTube playlist. This can be particularly useful for assessing
              the total watch time of course material, tutorial series, or any
              collection of videos grouped into a YouTube playlist.
            </p>
            <!-- Key Points Section -->
            <h5>Key Points</h5>
            <ul>
              <li>
                <strong>YouTube Data API v3:</strong> The script utilizes the
                YouTube Data API v3 to fetch data about videos and playlists.
              </li>
              <li>
                <strong>Python:</strong> The project is implemented in Python
                and makes use of various libraries and modules.
              </li>
              <li>
                <strong>API Key Management:</strong> The script has
                functionality to manage (save, update, and use) YouTube API
                keys.
              </li>
              <li>
                <strong>Progress Bar:</strong> Utilizes the tqdm library to
                display a progress bar during data retrieval and processing.
              </li>
              <li>
                <strong>Error Handling:</strong> Implements error handling for
                various issues like HTTP errors, connection issues, timeout
                errors, and more.
              </li>
              <li>
                <strong>Command-Line Usage:</strong> The script is designed to
                be used from the command line with various arguments for user
                convenience.
              </li>
            </ul>
            <!-- Links -->
            <h5>Explore Further</h5>
            <ul>
              <!-- GitHub Link -->
              <li>
                <strong>GitHub Repository:</strong>
                <a
                  href="https://github.com/cmsolson75/YouTubePlaylistCalculator"
                  target="_blank"
                  >YouTubePlaylistCalculator</a
                >
              </li>
            </ul>
          </div>
          <!-- Footer for line at bottom -->
          <div class="modal-footer"></div>
        </div>
      </div>
    </div>

    <!-- Modal for Socket Morse Chat App -->
    <div
      class="modal fade"
      id="project6Modal"
      tabindex="-1"
      aria-labelledby="project6ModalLabel"
      aria-hidden="true"
    >
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="modal-header">
            <!-- Title -->
            <h5 class="modal-title" id="project6ModalLabel">
              Socket Morse Chat App
            </h5>
            <!-- Exit Button -->
            <button
              type="button"
              class="btn-close"
              data-bs-dismiss="modal"
              aria-label="Close"
            ></button>
          </div>
          <!-- Modal Body -->
          <div class="modal-body">
            <!-- Overview Section -->
            <h5>Overview</h5>
            <p>
              The Socket Morse Chat App is a communication application developed
              in Python that allows users to send and receive messages in Morse
              code through a client-server architecture. The application
              translates text messages into Morse code and plays the
              corresponding audio (dots and dashes) to the users.
            </p>
            <!-- Key Points Section -->
            <h5>Key Points</h5>
            <ul>
              <li>
                <strong>Client-Server Architecture:</strong> The application
                operates on a client-server model where multiple clients can
                connect to a server and communicate with each other.
              </li>
              <li>
                <strong>Socket Programming:</strong> Utilizes Python's socket
                programming to handle the communication between the server and
                clients.
              </li>
              <li>
                <strong>Multithreading:</strong> Implements threading to manage
                multiple clients and handle sending and receiving messages
                simultaneously.
              </li>
              <li>
                <strong>Morse Code Translation and Playback:</strong> Translates
                text messages into Morse code and plays the corresponding sounds
                using audio files.
              </li>
            </ul>
            <!-- Links -->
            <h5>Explore Further</h5>
            <ul>
              <!-- Github Repo Link -->
              <li>
                <strong>Github Repository:</strong>
                <a
                  href="https://github.com/cmsolson75/SocketMorseChatApp"
                  target="_blank"
                  >SocketMorseChatApp</a
                >
              </li>
            </ul>
          </div>
          <!-- Footer for line -->
          <div class="modal-footer"></div>
        </div>
      </div>
    </div>

    <!-- Music Section -->
    <section class="music-section bg-light py-5" id="music">
      <div class="container">
        <h2 class="text-center mb-4">Generative Music</h2>

        <!-- Music Player Container -->
        <div class="music-players-container">
          <!-- Song I'm A Little Too Young to Be This Old -->
          <article class="music-player mb-3 p-3 border rounded">
            <!-- Title -->
            <h5 class="mb-2">
              I'm A Little Too Young to Be This Old | Almost Human | AI Song
              Competition
            </h5>
            <!-- Audio Player -->
            <audio controls class="w-100" id="audio1">
              <source src="audio/ToYoung.mp3" type="audio/mp3" />
              Your browser does not support the audio element.
            </audio>
          </article>

          <!-- Song Stay With Me -->
          <article class="music-player mb-3 p-3 border rounded">
            <!-- Title -->
            <h5 class="mb-2">
              Stay With Me | Exploring Audio Diffusion Synthesis | Berklee
              College of Music
            </h5>
            <!-- Audio Player -->
            <audio controls class="w-100" id="audio2">
              <source src="audio/StayWithMe.mp3" type="audio/mp3" />
              Your browser does not support the audio element.
            </audio>
          </article>

          <!-- Song Floating Textures -->
          <article class="music-player mb-3 p-3 border rounded">
            <!-- Title -->
            <h5 class="mb-2">
              Floating Textures | Exploring Audio Diffusion Synthesis | Berklee
              College of Music
            </h5>
            <!-- Audio Player -->
            <audio controls class="w-100" id="audio3">
              <source src="audio/FloatingTextures.mp3" type="audio/mp3" />
              Your browser does not support the audio element.
            </audio>
          </article>
        </div>
      </div>
    </section>

    <!-- Education Section -->
    <section class="p-5 bg-light" id="education">
      <div class="container py-5 bg-dark text-white">
        <h2 class="text-center mb-5 fw-bold">Education</h2>

        <div class="row justify-content-center">
          <div class="col-md-6">
            <div class="card bg-secondary text-white">
              <div class="card-body">
                <!-- Title -->
                <h3 class="fw-bold">Bachelor's Degree in Music</h3>
                <!-- School -->
                <p><strong>Berklee College of Music</strong></p>
                <ul class="list-unstyled">
                  <!-- Major -->
                  <li>
                    <strong>Major:</strong> Electronic Production and Design,
                    with a specialization in DSP and AI Music Systems
                  </li>
                  <!-- Minor -->
                  <li><strong>Minor:</strong> Computer Programming</li>
                </ul>
              </div>
            </div>
          </div>
        </div>
      </div>
      <!-- Online Coursework Section -->
      <div class="container py-5 bg-light text-dark">
        <h2 class="text-center mb-5 fw-bold">Online Coursework</h2>
        <div class="row">
          <!-- Mathematics for Machine Learning Specialization Section -->
          <div class="col-md-6 mb-4">
            <div class="card bg-light">
              <div class="card-body">
                <!-- Title -->
                <h4 class="card-title fw-bold text-dark">
                  Mathematics for Machine Learning Specialization
                </h4>
                <!-- List of platform & date of completion -->
                <ul class="list-unstyled text-secondary">
                  <!-- Platform -->
                  <li><strong>Platform:</strong> Coursera</li>
                  <!-- Completion Date -->
                  <li><strong>Completion Date:</strong> 2023</li>
                </ul>
              </div>
            </div>
          </div>
          <!-- Deep Learning Specialization Section  -->
          <div class="col-md-6 mb-4">
            <div class="card bg-light">
              <div class="card-body">
                <!-- Title -->
                <h4 class="card-title fw-bold text-dark">
                  Deep Learning Specialization
                </h4>
                <!-- List of platform & date of completion -->
                <ul class="list-unstyled text-secondary">
                  <!-- Platform -->
                  <li><strong>Platform:</strong> Coursera</li>
                  <!-- Completion Date -->
                  <li><strong>Completion Date:</strong> 2022</li>
                </ul>
              </div>
            </div>
          </div>
          <!-- Complete TensorFlow 2 and Keras Bootcamp Section -->
          <div class="col-md-6 mb-4">
            <div class="card bg-light">
              <div class="card-body">
                <!-- Title -->
                <h4 class="card-title fw-bold text-dark">
                  Complete TensorFlow 2 and Keras Bootcamp
                </h4>
                <!-- List of platform & date of completion -->
                <ul class="list-unstyled text-secondary">
                  <!-- Platform -->
                  <li><strong>Platform:</strong> Udemy</li>
                  <!-- Completion Date -->
                  <li><strong>Completion Date:</strong> 2021</li>
                </ul>
              </div>
            </div>
          </div>
          <!-- The Complete Python Bootcamp: From Zero to Hero -->
          <div class="col-md-6 mb-4">
            <div class="card bg-light">
              <div class="card-body">
                <!-- Title -->
                <h4 class="card-title fw-bold text-dark">
                  The Complete Python Bootcamp: From Zero to Hero
                </h4>
                <!-- List of platform & date of completion -->
                <ul class="list-unstyled text-secondary">
                  <!-- Platform -->
                  <li><strong>Platform:</strong> Udemy</li>
                  <!-- Completion Date -->
                  <li><strong>Completion Date:</strong> 2020</li>
                </ul>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Contact Section -->
    <footer class="p-5 text-white bg-dark" id="contact">
      <h2>Contact</h2>
      <a
        href="https://www.linkedin.com/in/cameron-olson-aaba85201/
        "
        class="text-white m-2"
        target="_blank"
        ><i class="fab fa-linkedin"></i
      ></a>
      <a href="mailto:cmsolson75@gmail.com" class="text-white m-2"
        ><i class="fas fa-envelope"></i
      ></a>
      <p>
        Thank you for visiting my portfolio. Feel free to connect with me on
        LinkedIn or send me an email.
      </p>
    </footer>

    <!-- JS Scripts -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js" defer></script>
    <script src="script.js" defer></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.10.2/dist/umd/popper.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.min.js"></script>
  </body>
</html>
